# ============================================
# INFRAGUARD OUMI TRAINING - FIXED VERSION
# ============================================

# ============ CELL 1: Install Dependencies ============
!pip install torch transformers datasets accelerate -q
print("âœ… Installation complete!")

# ============ CELL 2: Upload Training Data ============
from google.colab import files
print("Upload train.jsonl and val.jsonl files:")
uploaded = files.upload()

# ============ CELL 3: Verify Data ============
import json

def count_lines(filepath):
    with open(filepath) as f:
        return sum(1 for _ in f)

print(f"Training examples: {count_lines('train.jsonl')}")
print(f"Validation examples: {count_lines('val.jsonl')}")

with open('train.jsonl') as f:
    sample = json.loads(f.readline())
    print(f"\nSample prompt:\n{sample['prompt'][:300]}...")
    print(f"\nResponse: {sample['response']}")
    print(f"Reward: {sample['reward']}")

# ============ CELL 4: Train Model ============
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch
import json
import os

# Disable wandb completely
os.environ["WANDB_DISABLED"] = "true"

# Load model (use float16 instead of bfloat16)
model_name = "HuggingFaceTB/SmolLM2-360M-Instruct"
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # Changed from bfloat16
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
print("âœ… Model loaded!")

# Load data
def load_jsonl(filepath):
    with open(filepath) as f:
        return [json.loads(line) for line in f]

train_data = load_jsonl('train.jsonl')
train_positive = [d for d in train_data if d['reward'] > 0]
print(f"âœ… Positive training examples: {len(train_positive)}")

# Prepare dataset
texts = [f"{ex['prompt']}\n\nAction: {ex['response']}" for ex in train_positive]
encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors="pt")

class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, enc):
        self.enc = enc
    def __len__(self):
        return len(self.enc['input_ids'])
    def __getitem__(self, idx):
        return {
            'input_ids': self.enc['input_ids'][idx],
            'attention_mask': self.enc['attention_mask'][idx],
            'labels': self.enc['input_ids'][idx]
        }

# Training with fixes
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="./infraguard-model",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        learning_rate=2e-5,
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        bf16=True,           # Changed from fp16=True
        report_to="none"     # Disable wandb
    ),
    train_dataset=SimpleDataset(encodings)
)

print("ðŸš€ Starting training...")
trainer.train()
trainer.save_model("./infraguard-model-final")
print("âœ… Training complete!")

# ============ CELL 5: Download Model ============
from google.colab import files

!zip -r infraguard-model.zip infraguard-model-final/
files.download('infraguard-model.zip')

# ============ CELL 6: Test Model ============
test_prompt = """You are an SRE AI agent. Select the best action for:
- Type: crash_loop
- Severity: critical
- Pod: app-api-1234
- Namespace: demo-apps

Available Actions: restart_pod, scale_horizontal, increase_memory_limit, rollback_deployment, escalate_to_human

Best action:"""

inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
print("Model response:")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
